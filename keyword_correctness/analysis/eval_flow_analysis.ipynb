{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Evaluation Run Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import sys\n",
    "sys.path.append('./../common')\n",
    "\n",
    "from pf_sdk_utils import PromptFlowUtils\n",
    "\n",
    "pf_utils = PromptFlowUtils()\n",
    "pf_utils.initialize_pf_client(cloud = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_or_display_outputs(executed_runs):\n",
    "    from output_utils import download_output\n",
    "    output_runs_dir = \"./runs\"\n",
    "    for run in executed_runs:\n",
    "        if isinstance(run, str):\n",
    "            output_file_path = f\"{output_runs_dir}/{run}\"\n",
    "        else:\n",
    "            output_file_path = f\"{output_runs_dir}/{run.name}\"\n",
    "        download_output(run,output_file_path)\n",
    "    return f\"{output_file_path}.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_run_name = \"prasann_evaluation_experiment_step2_variant_0_28200658\"\n",
    "\n",
    "evaluation_run  = pf_utils.get_run(evaluation_run_name)\n",
    "eval_out_file = download_or_display_outputs([evaluation_run])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read source dataset and evaluation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "eval_out_df = pd.read_json(eval_out_file, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode the evaluation output to get the individual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the JSON objects into separate columns\n",
    "eval_scores_df = eval_out_df['scores'].apply(pd.Series)\n",
    "\n",
    "# Concatenate the original DataFrame with the new `json_df`\n",
    "eval_out_df = pd.concat([eval_out_df, eval_scores_df], axis=1).drop('scores', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the false positives rows\n",
    "eval_df_fp = eval_out_df[eval_out_df[\"false_positive\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the big parse error rows\n",
    "eval_df_error = eval_out_df[eval_out_df[\"pred_big_parse_error\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_error['pred_big_parse_error_msg'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze individual big parse errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new data-frame to compare the keywords between the predictions and the ground truth\n",
    "import json\n",
    "\n",
    "def create_comparison_df(input_df: pd.DataFrame):\n",
    "    compare_keywords_df = pd.DataFrame(columns=['fsn', 'seller', 'llms', 'missing', 'extra'])\n",
    "\n",
    "    for rows in input_df.iterrows():\n",
    "        predictions_str = rows[1][\"inputs.predictions_str\"].replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        predictions = json.loads(predictions_str)\n",
    "        prediction_keywords = []\n",
    "        for prediction in predictions:\n",
    "            prediction_keywords.append(prediction[\"keyword\"])\n",
    "        keywords = list(rows[1][\"inputs.ground_truth\"].keys())\n",
    "        missing_keywords = [item for item in keywords if item not in prediction_keywords]\n",
    "        extra_keywords = [item for item in prediction_keywords if item not in keywords]\n",
    "\n",
    "        new_df = pd.DataFrame({'fsn': [rows[1][\"inputs.fsn\"]] ,'seller': [keywords], 'llms': [prediction_keywords], 'missing': [missing_keywords], 'extra': [extra_keywords]})\n",
    "        compare_keywords_df = pd.concat([compare_keywords_df, new_df], ignore_index=True)\n",
    "\n",
    "    return compare_keywords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsing_error_df = eval_df_error[eval_df_error[\"pred_big_parse_error_msg\"] == \"Error parsing predictions as json\"]\n",
    "print(parsing_error_df[\"inputs.predictions_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_error_msg = eval_df_error[eval_df_error[\"pred_big_parse_error_msg\"] == \"Predictions keywords different than GT keywords\"]\n",
    "compare_prediction_diff_df = create_comparison_df(eval_df_error_msg)\n",
    "len(compare_prediction_diff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_error_not_alist = eval_df_error[eval_df_error[\"pred_big_parse_error_msg\"] == \"Predictions not a list or # of keywords in predictions is different than # of GT keywords.\"]\n",
    "compare_keywords_not_a_list_df = create_comparison_df(eval_df_error_not_alist)\n",
    "len(compare_keywords_not_a_list_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pf-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
