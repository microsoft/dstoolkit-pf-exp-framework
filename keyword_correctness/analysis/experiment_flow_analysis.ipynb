{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of the GPT 4v csv\n",
    "data_file = \"../keyword_correctness/runs/prasann_experiment_step_1_variant_0_27204515.csv\"\n",
    "\n",
    "# log file of the GPT 4v run (it will be .txt file, simply rename it to .log)\n",
    "log_file = '../keyword_correctness/executionlogs.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_df = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_rows = results_df[results_df['Status'] == 'Failed']\n",
    "sorted_failed_rows = failed_rows.sort_values(by='inputs.row_index')\n",
    "# print(sorted_failed_rows)\n",
    "\n",
    "len(sorted_failed_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the log file into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "def parse_log_file(log_file_path):\n",
    "    with open(log_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        match = re.match(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} \\+\\d{4})\\s+(\\d+)\\s+(\\w+)\\s+(\\w+)\\s+(.*)', line)\n",
    "        if match:\n",
    "            data.append(match.groups())\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['timestamp', 'process_id', 'module', 'log_level', 'message'])\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp']).dt.tz_localize(None)\n",
    "\n",
    "    # Set data types for each column\n",
    "    df = df.astype({\n",
    "        'timestamp': 'datetime64[ns]',  # convert to datetime type\n",
    "        'process_id': 'int32',  # convert to integer type\n",
    "        'module': 'string',  # convert to string type\n",
    "        'log_level': 'category',  # convert to categorical type\n",
    "        'message': 'string'\n",
    "    })\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    return df\n",
    "\n",
    "log_df = parse_log_file(log_file_path=log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "error_rows = log_df[log_df['log_level'] == 'ERROR'].copy()\n",
    "\n",
    "# set the failed line number from the log message\n",
    "error_rows['failed_line_num'] = error_rows['message'].str.extract(r'line (\\d+) failed')\n",
    "error_rows = error_rows.dropna(subset=['failed_line_num'])\n",
    "error_rows['failed_line_num'] = error_rows['failed_line_num'].astype('int64')\n",
    "\n",
    "error_rows = error_rows.dropna(subset=['timestamp', 'process_id', 'module', 'log_level', 'message', 'failed_line_num'])\n",
    "\n",
    "bad_request_rows = error_rows[error_rows['message'].str.contains('BadRequestError', regex=False)]\n",
    "\n",
    "errors_not_bad_requests = error_rows[~error_rows['message'].str.contains('BadRequestError', regex=False)]\n",
    "\n",
    "print(f\"Total errors: {len(error_rows)}\")\n",
    "print(f\"Total bad requests: {len(bad_request_rows)}\")\n",
    "print(f\"Not bad requests: {len(errors_not_bad_requests)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_logs_and_error_dfs = sorted_failed_rows.merge(error_rows, left_on='Line number', right_on='failed_line_num', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_logs_and_error_dfs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errored FSNs that are not in the logs\n",
    "\n",
    "rows_not_in_logs = combine_logs_and_error_dfs[combine_logs_and_error_dfs['failed_line_num'].isna() | combine_logs_and_error_dfs['failed_line_num'].eq('')]\n",
    "rows_not_in_logs.columns\n",
    "\n",
    "print(rows_not_in_logs[['Line number','inputs.fsn']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bad_request_rows['message'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
